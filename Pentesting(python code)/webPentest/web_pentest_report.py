# web_pentest_report.py (Main Entry Point)

import argparse
import socket
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse
import sys
import os

# Add path to ai_engine (relative to project root)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
AI_ENGINE_PATH = os.path.join(BASE_DIR, "ai_engine")
sys.path.append(AI_ENGINE_PATH)

from ai_engine.ai_brain import AIBrain

from web_scanner import run_web_scan  # OWASP ZAP-based scanner
from web_osint import gather_main_osint_info,gather_social_osint_info, shodan_lookup, dns_lookup  # HTTP headers, DNS, Shodan, etc.
from ai_summary_web import generate_web_ai_summary
from export_report import export_to_markdown, export_to_pdf
from web_scrapping import scrape_bio, discover_social_links  # optional: scraping social bios

def run_web_vuln_scan(url, export_format="both", social_urls=None):
    print(f"[+] Starting vulnerability scan on: {url}")

    vuln_results = run_web_scan(url)

    #vuln_scanning complex pdf version
    export_to_pdf(vuln_results,'C:\\Users\\2331025\\Pentesting(python code)\\Report(Vuln_Scanning(simplified+complex))','web','complex')

    summary = generate_web_ai_summary(vuln_results)

    export_to_pdf(summary,'C:\\Users\\2331025\\Pentesting(python code)\\Report(Vuln_Scanning(simplified+complex))','web','simplified')

    # Return all relevant info to pass later to attack phase
    return {
        "url": url,
        "vulnerabilities": vuln_results,
        "summary": summary,
        "export_format": export_format,
        "social_urls": social_urls or [],
    }


def run_web_attack_phase(vuln_scan_data, can_proceed=False):
    if not can_proceed:
        print("[!] User declined to proceed with pentest attack after scanning.")
        return {
            "attack_performed": False,
            "message": "Attack aborted by user.",
            "summary": vuln_scan_data["summary"],
            "vulnerabilities": vuln_scan_data["vulnerabilities"],
        }

    url = vuln_scan_data["url"]
    vuln_results = vuln_scan_data["vulnerabilities"]
    export_format = vuln_scan_data["export_format"]
    social_urls = vuln_scan_data["social_urls"]

    print(f"[+] Proceeding with attack and enrichment for: {url}")

    osint_results = gather_main_osint_info(url)
    domain = urlparse(url).netloc
    dns_info = dns_lookup(domain)

    try:
        ip = socket.gethostbyname(domain)
    except Exception as e:
        ip = None
        print(f"[OSINT] Could not resolve IP: {e}")

    shodan_info = shodan_lookup(ip) if ip else {"error": "IP resolution failed"}

    combined_osint = {
        **osint_results,
        "dns": dns_info,
        "shodan": shodan_info
    }

    def is_scan_fruitful(scan_results):
        return bool(scan_results and "vulnerability" in scan_results.lower())

    if not is_scan_fruitful(generate_web_ai_summary(vuln_results)):
        if not social_urls:
            social_urls = discover_social_links(url)

        if social_urls:
            social_osint = gather_social_osint_info(social_urls)
            bios = {s_url: scrape_bio(s_url) for s_url in social_urls}
            combined_osint.update(social_osint)
            combined_osint["bios"] = bios

    summary = generate_web_ai_summary(vuln_results, osint=combined_osint)

    file_prefix = url.replace("http://", "").replace("https://", "").replace("/", "_")
    exported_files = []
    if export_format in ["md", "both"]:
        md_file = export_to_markdown(summary, 'C:\\Users\\2331025\\Pentesting(python code)\\Report(Vuln_Scanning(simplified+complex))', p_type="web",context='')
        exported_files.append(md_file)
    if export_format in ["pdf", "both"]:
        pdf_file = export_to_pdf(summary, 'C:\\Users\\2331025\\Pentesting(python code)\\Report(Vuln_Scanning(simplified+complex))','web')
        exported_files.append(pdf_file)

    ai = AIBrain(domain="web")
    ai.exploit_from_scan(vuln_results)
    ai_summary = ai.generate_summary()

    print(f"[✓] Final report generated for {url}")

    return {
        "attack_performed": True,
        "summary": summary,
        "ai_summary": ai_summary,
        "exported_files": exported_files,
        "vulnerabilities": vuln_results,
    }

#
# def is_scan_inconclusive(vuln_results):
#     """Decide if scan is too weak to report meaningfully"""
#     return not vuln_results or len(vuln_results) < 1  # Customize threshold
#
# def is_scan_fruitful(scan_results):
#     # You define what “fruitful” means.
#     # Here: at least 1 risk/vulnerability string exists.
#     return bool(scan_results and "vulnerability" in scan_results.lower())
#
# def scan_and_report(url, export_format, social_urls=None):
#     print(f"[+] Starting scan on: {url}")
#
#     # 1. Vulnerability Scan
#     vuln_results = run_web_scan(url)
#
#     # 2. OSINT from main domain
#     osint_results = gather_main_osint_info(url)
#
#     # 3. DNS and Shodan
#     domain = urlparse(url).netloc
#     dns_info = dns_lookup(domain)
#     try:
#         ip = socket.gethostbyname(domain)
#     except Exception as e:
#         ip = None
#         print(f"[OSINT] Could not resolve IP: {e}")
#     shodan_info = shodan_lookup(ip) if ip else {"error": "IP resolution failed"}
#
#     # Initial combined OSINT
#     combined_osint = {
#         **osint_results,
#         "dns": dns_info,
#         "shodan": shodan_info
#     }
#
#     # 4. First AI Risk Summary
#     summary = generate_web_ai_summary(vuln_results, osint=combined_osint)
#
#     # 5. If scan weak, try enriching via social media
#     if not is_scan_fruitful(summary):
#         print("[!] Scan inconclusive. Enriching via social discovery...")
#
#         # Discover social URLs if not provided
#         if not social_urls:
#             social_urls = discover_social_links(url)
#
#         if social_urls:
#             print(f"[+] Using social URLs: {social_urls}")
#             social_osint = gather_social_osint_info(social_urls)
#
#             # Optional bios scraping
#             bios = {}
#             for s_url in social_urls:
#                 bio = scrape_bio(s_url)
#                 bios[s_url] = bio
#                 print(f"[BIO] {s_url} → {bio}")
#
#             # Update combined OSINT
#             combined_osint.update(social_osint)
#             combined_osint["bios"] = bios
#
#             # Retry AI summary with enriched data
#             print("Retrying AI analysis with enriched OSINT...")
#             summary = generate_web_ai_summary(vuln_results, osint=combined_osint)
#
#     # 6. Report Export
#     file_prefix = url.replace("http://", "").replace("https://", "").replace("/", "_")
#     if export_format in ["md", "both"]:
#         export_to_markdown(summary, file_path=f"{file_prefix}_report.md", context="web")
#     if export_format in ["pdf", "both"]:
#         export_to_pdf(summary, file_path=f"{file_prefix}_report.pdf")
#
#     print(f"[✓] Final report generated for {url}")
#
#     ai = AIBrain(domain="web")
#     ai.exploit_from_scan(vuln_results)
#
#     ai_summary = ai.generate_summary()
#
# def main():
#     parser = argparse.ArgumentParser(description="Web Pentest + OSINT + AI Reporting")
#     parser.add_argument("--url", nargs='+', required=True, help="List of main web targets")
#     parser.add_argument("--social", nargs='*', help="Optional social media URLs")
#     parser.add_argument("--export", choices=["md", "pdf", "both"], default="both")
#
#     args = parser.parse_args()
#
#     executor = ThreadPoolExecutor(max_workers=5)
#
#     #Start scanning main targets with priority
#     for main_url in args.url:
#         executor.submit(scan_and_report, main_url, args.export, social_urls=None)
#
#     executor.shutdown(wait=True)
#     print("[+] All scans complete.")
#
# if __name__ == "__main__":
#     main()
